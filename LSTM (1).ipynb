{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwvJTcNmQtgY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from transformers import BertTokenizer\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v1p0AgnpY7Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52b037e-0925-43f8-baf1-717713c01c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('drive/My Drive/trac2_CONVT_train.csv', usecols = range(12), on_bad_lines='skip')\n",
        "dev_data = pd.read_csv('drive/My Drive/trac2_CONVT_dev.csv', usecols=range(12), on_bad_lines='skip')\n"
      ],
      "metadata": {
        "id": "CtDjtdE9TeBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['Emotion', 'EmotionalPolarity', 'Empathy']:\n",
        "    train_data[col] = pd.to_numeric(train_data[col], errors='coerce')\n",
        "    dev_data[col] = pd.to_numeric(dev_data[col], errors='coerce')\n",
        "\n",
        "train_data = train_data.dropna(subset=['Emotion', 'EmotionalPolarity', 'Empathy']).reset_index(drop=True)\n",
        "dev_data = dev_data.dropna(subset=['Emotion', 'EmotionalPolarity', 'Empathy']).reset_index(drop=True)\n",
        "\n",
        "# Sample smaller datasets for quick training (adjust or remove for full dataset)\n",
        "#train_data = train_data.sample(n=4800, random_state=4400)\n",
        "#dev_data = dev_data.sample(n=960, random_state=4400)"
      ],
      "metadata": {
        "id": "0sK-p0SYTeEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.data.iloc[index]['text']\n",
        "        labels = self.data.iloc[index][['Emotion', 'EmotionalPolarity', 'Empathy']].values.astype(float)\n",
        "        encoding = self.tokenizer(text,\n",
        "                                  max_length=self.max_length,\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  return_tensors='pt')\n",
        "        return {'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(labels, dtype=torch.float)}"
      ],
      "metadata": {
        "id": "mydd9T78TeIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = ConversationDataset(train_data, tokenizer)\n",
        "dev_dataset = ConversationDataset(dev_data, tokenizer)\n",
        "\n",
        "# Wrap datasets in DataLoader objects\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "j_3-dXDrTeLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(glove_path, tokenizer, embedding_dim=300):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    # Prepare embedding matrix\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    embedding_matrix = np.random.normal(size=(vocab_size, embedding_dim))  # Random initialization\n",
        "    for word, idx in tokenizer.get_vocab().items():\n",
        "        if idx >= vocab_size:  # Ensure we stay within vocab size\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "BRSBACrmNlyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "PTX362kYNmMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path = \"glove.6B.300d.txt\"\n",
        "embedding_dim = 300\n",
        "embedding_matrix = load_glove_embeddings(glove_path, tokenizer, embedding_dim)\n"
      ],
      "metadata": {
        "id": "ku7_yX3aNr9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_outputs, pad_idx, embedding_matrix=None):\n",
        "        super(LSTMAttentionModel, self).__init__()\n",
        "\n",
        "        # Embedding layer with pretrained embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
        "            self.embedding.weight.requires_grad = False  # Freeze embeddings\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_outputs)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Embedding lookup\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        embeddings = embeddings * attention_mask.unsqueeze(-1)\n",
        "\n",
        "        # LSTM outputs\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "\n",
        "        # Attention weights\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        context_vector = self.dropout(context_vector)\n",
        "        outputs = self.fc(context_vector)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "tJ2Lly6FTeNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 256\n",
        "num_outputs = 3  # Emotion, Polarity, Empathy\n",
        "pad_idx = tokenizer.pad_token_id\n",
        "\n",
        "# Initialize the LSTM model\n",
        "lstm_attention_model = LSTMAttentionModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embed_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_outputs=num_outputs,\n",
        "    pad_idx=pad_idx,\n",
        "    embedding_matrix=embedding_matrix\n",
        ")"
      ],
      "metadata": {
        "id": "XRBcSbj7TeQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "lstm_attention_model.to(device)\n",
        "\n",
        "optimizer = AdamW(lstm_attention_model.parameters(), lr=0.005)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "_KPMlwpGYYmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    lstm_attention_model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = lstm_attention_model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "tFXdeQRPYYoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee9fb1c-e218-4b42-b2b7-25b627aa344b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 0.4950\n",
            "Epoch 2: Train Loss = 0.3863\n",
            "Epoch 3: Train Loss = 0.3526\n",
            "Epoch 4: Train Loss = 0.3240\n",
            "Epoch 5: Train Loss = 0.3027\n",
            "Epoch 6: Train Loss = 0.2789\n",
            "Epoch 7: Train Loss = 0.2607\n",
            "Epoch 8: Train Loss = 0.2409\n",
            "Epoch 9: Train Loss = 0.2314\n",
            "Epoch 10: Train Loss = 0.2114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_attention_model.eval()\n",
        "predictions = []\n",
        "val_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = lstm_attention_model(input_ids, attention_mask)\n",
        "        predictions.append(outputs.cpu().numpy())\n",
        "        val_labels.append(labels.cpu().numpy())\n",
        "\n",
        "# Flatten predictions and labels into arrays\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "val_labels = np.concatenate(val_labels, axis=0)"
      ],
      "metadata": {
        "id": "Au1KYn_lYYqv"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearson_emotion = pearsonr(val_labels[:, 0], predictions[:, 0])[0]\n",
        "pearson_emotional_polarity = pearsonr(val_labels[:, 1], predictions[:, 1])[0]\n",
        "pearson_empathy = pearsonr(val_labels[:, 2], predictions[:, 2])[0]\n",
        "average_pearson = (pearson_emotion + pearson_emotional_polarity + pearson_empathy) / 3\n",
        "print(\"Emotion Intensity Pearson Score:\", pearson_emotion)\n",
        "print(\"Emotional Polarity Pearson Score:\", pearson_emotional_polarity)\n",
        "print(\"Empathy Pearson Score:\", pearson_empathy)\n",
        "print(\"Average Pearson Score:\", average_pearson)\n"
      ],
      "metadata": {
        "id": "T_H-_pnOcFBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d98d9e-86bc-4465-ce24-44e65e87cc52"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emotion Intensity Pearson Score: 0.061023749641554055\n",
            "Emotional Polarity Pearson Score: 0.08659075659901935\n",
            "Empathy Pearson Score: 0.14882440724852586\n",
            "Average Pearson Score: 0.09881297116303307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('drive/My Drive/goldstandard_CONVT.csv',on_bad_lines='skip')\n"
      ],
      "metadata": {
        "id": "3DOVPGbzcFJW"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['Emotion', 'EmotionalPolarity', 'Empathy']:\n",
        "    test_data[col] = pd.to_numeric(test_data[col], errors='coerce')\n",
        "\n",
        "test_data = test_data.dropna(subset=['Emotion', 'EmotionalPolarity', 'Empathy']).reset_index(drop=True)\n",
        "\n",
        "# Create a test dataset and DataLoader\n",
        "test_dataset = ConversationDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "lstm_attention_model.eval()\n",
        "test_predictions = []\n",
        "test_labels = []"
      ],
      "metadata": {
        "id": "dAxQBeo6cFL8"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = lstm_attention_model(input_ids, attention_mask)\n",
        "        test_predictions.append(outputs.cpu().numpy())\n",
        "        test_labels.append(labels.cpu().numpy())"
      ],
      "metadata": {
        "id": "Aa7h5HuhcFOS"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = np.concatenate(test_predictions, axis=0)\n",
        "test_labels = np.concatenate(test_labels, axis=0)"
      ],
      "metadata": {
        "id": "rjOo_haDcFQn"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearson_emotion = pearsonr(test_labels[:, 0], test_predictions[:, 0])[0]\n",
        "pearson_emotional_polarity = pearsonr(test_labels[:, 1], test_predictions[:, 1])[0]\n",
        "pearson_empathy = pearsonr(test_labels[:, 2], test_predictions[:, 2])[0]\n",
        "average_pearson = (pearson_emotion + pearson_emotional_polarity + pearson_empathy) / 3\n",
        "\n",
        "print(\"Test Set Evaluation:\")\n",
        "print(\"Emotion Intensity Pearson Score:\", pearson_emotion)\n",
        "print(\"Emotional Polarity Pearson Score:\", pearson_emotional_polarity)\n",
        "print(\"Empathy Pearson Score:\", pearson_empathy)\n",
        "print(\"Average Pearson Score:\", average_pearson)"
      ],
      "metadata": {
        "id": "p_-68qkocFSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5413c818-05ee-4deb-8dec-3051703658d5"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation:\n",
            "Emotion Intensity Pearson Score: 0.5585666899727699\n",
            "Emotional Polarity Pearson Score: 0.5482412580472854\n",
            "Empathy Pearson Score: 0.49180286080332924\n",
            "Average Pearson Score: 0.5328702696077948\n"
          ]
        }
      ]
    }
  ]
}