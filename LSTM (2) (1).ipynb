{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwvJTcNmQtgY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import BertTokenizer\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1p0AgnpY7Ep",
    "outputId": "e52b037e-0925-43f8-baf1-717713c01c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtDjtdE9TeBC"
   },
   "outputs": [],
   "source": [
    "# get rid of rows with data outside first 12 cols, skip parsing errors\n",
    "train_data = pd.read_csv('drive/My Drive/trac2_CONVT_train.csv', usecols = range(12), on_bad_lines='skip')\n",
    "dev_data = pd.read_csv('drive/My Drive/trac2_CONVT_dev.csv', usecols=range(12), on_bad_lines='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sK-p0SYTeEF"
   },
   "outputs": [],
   "source": [
    "# convert to numeric vals\n",
    "for col in ['Emotion', 'EmotionalPolarity', 'Empathy']:\n",
    "    train_data[col] = pd.to_numeric(train_data[col], errors='coerce')\n",
    "    dev_data[col] = pd.to_numeric(dev_data[col], errors='coerce')\n",
    "\n",
    "train_data = train_data.dropna(subset=['Emotion', 'EmotionalPolarity', 'Empathy']).reset_index(drop=True)\n",
    "dev_data = dev_data.dropna(subset=['Emotion', 'EmotionalPolarity', 'Empathy']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mydd9T78TeIx"
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index): #extract text, emotion intensity, polarity, empathy labels\n",
    "        text = self.data.iloc[index]['text']\n",
    "        labels = self.data.iloc[index][['Emotion', 'EmotionalPolarity', 'Empathy']].values.astype(float)\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  max_length=self.max_length,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  return_tensors='pt')\n",
    "\n",
    "        #tokenize into input IDs and attention masks\n",
    "        return {'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'labels': torch.tensor(labels, dtype=torch.float)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_3-dXDrTeLX"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #use BERT tokenizer\n",
    "\n",
    "# create dataset objects\n",
    "train_dataset = ConversationDataset(train_data, tokenizer)\n",
    "dev_dataset = ConversationDataset(dev_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRSBACrmNlyn"
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, tokenizer, embedding_dim=300):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    embedding_matrix = np.random.normal(size=(vocab_size, embedding_dim))  \n",
    "    for word, idx in tokenizer.get_vocab().items():\n",
    "        if idx >= vocab_size:  # stay within vocab size\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTX362kYNmMO"
   },
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "#!unzip -q glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ku7_yX3aNr9U"
   },
   "outputs": [],
   "source": [
    "glove_path = \"glove.840B.300d.txt\"\n",
    "embedding_dim = 300\n",
    "embedding_matrix = load_glove_embeddings(glove_path, tokenizer, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJ2Lly6FTeNt"
   },
   "outputs": [],
   "source": [
    "class LSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_outputs, pad_idx, embedding_matrix=None):\n",
    "        super(LSTMAttentionModel, self).__init__()\n",
    "\n",
    "        # using our pretrained embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "            self.embedding.weight.requires_grad = False  \n",
    "            \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # ensure that we use attention\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_outputs)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = embeddings * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "\n",
    "        # Attention weights\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        outputs = self.fc(context_vector)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRBcSbj7TeQU"
   },
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "num_outputs = 3  # Emotion, Polarity, Empathy\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "\n",
    "# initialize the LSTM \n",
    "lstm_attention_model = LSTMAttentionModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_outputs=num_outputs,\n",
    "    pad_idx=pad_idx,\n",
    "    embedding_matrix=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KPMlwpGYYmW"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lstm_attention_model.to(device)\n",
    "\n",
    "optimizer = AdamW(lstm_attention_model.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFXdeQRPYYoq",
    "outputId": "2ee9fb1c-e218-4b42-b2b7-25b627aa344b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.4950\n",
      "Epoch 2: Train Loss = 0.3863\n",
      "Epoch 3: Train Loss = 0.3526\n",
      "Epoch 4: Train Loss = 0.3240\n",
      "Epoch 5: Train Loss = 0.3027\n",
      "Epoch 6: Train Loss = 0.2789\n",
      "Epoch 7: Train Loss = 0.2607\n",
      "Epoch 8: Train Loss = 0.2409\n",
      "Epoch 9: Train Loss = 0.2314\n",
      "Epoch 10: Train Loss = 0.2114\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    lstm_attention_model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_attention_model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Au1KYn_lYYqv"
   },
   "outputs": [],
   "source": [
    "lstm_attention_model.eval()\n",
    "predictions = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = lstm_attention_model(input_ids, attention_mask)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        val_labels.append(labels.cpu().numpy())\n",
    "\n",
    "#flatten predictions and labels into arrays\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "val_labels = np.concatenate(val_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_H-_pnOcFBk",
    "outputId": "89d98d9e-86bc-4465-ce24-44e65e87cc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Intensity Pearson Score: 0.061023749641554055\n",
      "Emotional Polarity Pearson Score: 0.08659075659901935\n",
      "Empathy Pearson Score: 0.14882440724852586\n",
      "Average Pearson Score: 0.09881297116303307\n"
     ]
    }
   ],
   "source": [
    "pearson_emotion = pearsonr(val_labels[:, 0], predictions[:, 0])[0]\n",
    "pearson_emotional_polarity = pearsonr(val_labels[:, 1], predictions[:, 1])[0]\n",
    "pearson_empathy = pearsonr(val_labels[:, 2], predictions[:, 2])[0]\n",
    "average_pearson = (pearson_emotion + pearson_emotional_polarity + pearson_empathy) / 3\n",
    "print(\"Emotion Intensity Pearson Score:\", pearson_emotion)\n",
    "print(\"Emotional Polarity Pearson Score:\", pearson_emotional_polarity)\n",
    "print(\"Empathy Pearson Score:\", pearson_empathy)\n",
    "print(\"Average Pearson Score:\", average_pearson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "3DOVPGbzcFJW"
   },
   "outputs": [],
   "source": [
    "#use goldstandard test data\n",
    "test_data = pd.read_csv('drive/My Drive/goldstandard_CONVT.csv',on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "dAxQBeo6cFL8"
   },
   "outputs": [],
   "source": [
    "for col in ['Emotion', 'EmotionalPolarity', 'Empathy']:\n",
    "    test_data[col] = pd.to_numeric(test_data[col], errors='coerce')\n",
    "\n",
    "test_data = test_data.dropna(subset=['Emotion', 'EmotionalPolarity', 'Empathy']).reset_index(drop=True)\n",
    "\n",
    "test_dataset = ConversationDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "lstm_attention_model.eval()\n",
    "test_predictions = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Aa7h5HuhcFOS"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = lstm_attention_model(input_ids, attention_mask)\n",
    "        test_predictions.append(outputs.cpu().numpy())\n",
    "        test_labels.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "rjOo_haDcFQn"
   },
   "outputs": [],
   "source": [
    "test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "test_labels = np.concatenate(test_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_-68qkocFSc",
    "outputId": "5413c818-05ee-4deb-8dec-3051703658d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation:\n",
      "Emotion Intensity Pearson Score: 0.5585666899727699\n",
      "Emotional Polarity Pearson Score: 0.5482412580472854\n",
      "Empathy Pearson Score: 0.49180286080332924\n",
      "Average Pearson Score: 0.5328702696077948\n"
     ]
    }
   ],
   "source": [
    "pearson_emotion = pearsonr(test_labels[:, 0], test_predictions[:, 0])[0]\n",
    "pearson_emotional_polarity = pearsonr(test_labels[:, 1], test_predictions[:, 1])[0]\n",
    "pearson_empathy = pearsonr(test_labels[:, 2], test_predictions[:, 2])[0]\n",
    "average_pearson = (pearson_emotion + pearson_emotional_polarity + pearson_empathy) / 3\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"Emotion Intensity Pearson Score:\", pearson_emotion)\n",
    "print(\"Emotional Polarity Pearson Score:\", pearson_emotional_polarity)\n",
    "print(\"Empathy Pearson Score:\", pearson_empathy)\n",
    "print(\"Average Pearson Score:\", average_pearson)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
